

[
  
  
    
    
      {
        "title": "深入索引一点点",
        "excerpt": "对 CS 554 中索引类型的一些总结\n\n",
        "content": "对 CS 554 中索引类型的一些总结\n\n索引是什么？\n\n\n  On the most fundamental level, a database needs to do things: when you give it some data, it should store the data,and\nwhen you ask it again later, it should give the data back to you. – Designing Data-Intensive Applications\n\n\n在日均增长亿量数据的互联网时代，网络服务每时每刻都在与这些数据打交道，搜索、分析、转换等。时间就是成本，如何高效处理这些数据将成为难题。为减少查\n询数据库的开销，计算机科学家设计了各种算法来处理不同的场景，同时引进一种数据结构：索引(index)，来支持高效查询数据库中特定的值。索引大致的\n工作方式即是保存一些额外的元数据作为标记，帮助查找所需数据。如果需要在同一份数据中用不同的方式搜索，可能需要不同的索引，建在数据的不同部分上。索\n引是从主数据衍生的附加结构。许多数据库允许添加与删除索引，这不会影响数据的内容，只影响查询的性能。维护额外的结构会产生开销，特别是在写入时。任何\n类型的索引通常都会减慢写入速度，因为每次写入数据时都需要更新索引。因此，存储系统中重要的一个权衡：精心选择的索引加快了读查询的速度，但是每个索引\n都会拖慢写入速度。所以，数据库默认不会索引所有的内容，而需要使用者通过对应用查询模式的了解来手动选择索引，为应用带来最大收益，同时又不会引入超出\n必要开销的索引。\n\n优缺点\n\nPros:\n\n\n  提高查询性能，高效读取数据\n\n\nCons:\n\n\n  占用磁盘与内存空间\n  维护开销，减慢写入性能（插入、更新、删除）\n\n\n索引有哪些？\n\n索引大致分为两类：\n\n\n  有序索引（Sorted index）\n    \n      搜索 keys 有序存储在索引文件中\n      使用二分查找(binary search)快速搜索\n    \n  \n  哈希索引（Hashing index）\n    \n      搜索 keys 存储在哈希桶中\n      使用哈希函数(hash function)快速搜索\n    \n  \n\n\n其中，哈希索引（Hash index）是使用最广泛的一种索引，而有序索引的代表，则为 MySQL 中的 Primary index(an ordered index whose search\nkey is also the sort key used for sequential file)和 Secondary index(an ordered index whose search key is not the sort\nkey used for the sequential file)，还有其他形式的索引，比如 LevelDB 的 SSTable，ElasticSearch 中的倒排索引（Reverse index），\nBitMap，etc,.\n\n哈希索引 - Hashing Table\n\n\n  Hash Function: a function that maps a search key to an index between[0 .. B-1] (B = the size of the hash table)\n\n\n通常应用于键值数据（key-value Data）。在大多数编程语言中都能找到类似字典（dictionary）类型数据结构，通常用散列映射（hash map）或者\n哈希表（hash table）实现，具有 O(1)极高效的查找效率。\n\n数据存储形式\n\n\n\n\n  一个 bucket 对应一个 disk block\n  一个 bucket 包含 n 个（key，ptr）对\n\n\n存储与查找\n\n\nSteps\n\n\n  Compute the hash value h(W)\n  Read the disk block(s) of bucket h(W) into memory\n  Search (a linear search algorithm is sufficient because memory search is fast) the bucket h(W) for: W, RecordPtr(W)\n  Use RecordPtr(W) to access W on disk\n\n\nHashing animation\n\n\n\n扩容\n\n当哈希表插入许多值时，将会产生许多溢出块（哈希冲突），此时将会产生更多的磁盘读取操作，降低性能。通过增加哈希表的容量是一种普遍有效的解决方法，但\n是成本很高，通常需要重新映射所有 keys 值至新哈希表。因而提出两种动态哈希方法，当哈希表的大小改变时，都只需要重新映射小部分已存在的 keys。\n\n\n  可扩展哈希（Extensible Hashing）：能完全消除溢出数据的影响，但是哈希容量呈指数增长\n  线性哈希（Linear Hashing）：不能完全消除溢出数据的影响，但是哈希容量呈线性增长\n\n\n优缺点\n\nPros:\n\n\n  单数据行 O(1)操作性能\n\n\nCons:\n\n\n  对于范围查询和排序无法很好支持，最终可能导致全表扫描\n  需要足够内存放入散列表（在大量数据时，对于磁盘哈希映射，需要大量的随机访问 I/O，且无法高效处理 Resizing 和 Collision）\n  无法支持模糊搜索\n\n\n有序索引 - B-Tree and B+-tree\n\n根据公开资料，读取磁盘数据块通常在0.01 秒内，当计算数据库操作的延迟时，主要考虑磁盘随机访问次数。对于单一有序索引，使用二分查找算法，最糟糕的耗时为 O(logn)；如果是多层有序索引，能够极大降低磁盘 I/O 次数，这时，索引文件的层数将成为关键，这取决于目标 key 在索引中的位置（ground level）；如果 keys 越多，索引可能需要更多的层数。此时，B+ tree 能够根据索引数量动态判断索引的层数。\n\n\n索引层数与访问次数\n\n\n\n\n  图示三次磁盘访问即可查找到数据地址\n  因此，为降低磁盘 I/O 次数，必须降低索引层数\n\n\nB+ Tree 的定义\n\n\n\n\n  动态多层级(3 层可储存约 4 千万级数据)\n  平衡树（每个叶子结点到根结点距离相同）\n  每个节点含有多个数据地址（每个节点占据一个磁盘块，为页的整数倍（4KB），一个 block 可储存 340【4n+8(n+1) &lt;= 4096】个 keys，四层可保存 53T 数据）\n  叶子节点存储所有数据「index file」（密集索引）\n  叶子节点通过指针按顺序连接，即包含一个指向下一邻近叶节点的指针。\n\n\n叶子节点的结构\n\n\n\n\n  每个叶子节点(leaf node)存储在一个磁盘块(disk block)\n  索引键即为想要快速查找的值\n  叶子节点的指针（除最后的指针）为关联索引键记录数据库数据（database record）的地址\n  叶子节点从左至右链接\n\n\n查找 B-tree\n\n\n  起始在根结点使用线性搜索查询下一个节点\n\n  在内部节点中重复该步骤\n  当到达叶节点时，线性搜索目标 key\n\n\n\n插入新值\n\n\n\n\n  若叶节点有空余空间，查找对应叶节点，位移 keys 并插入目标 key\n  若叶节点空间已满，查找对应叶节点，插入目标 key，并等半分裂为两个节点，在父节点插入两个新叶节点中的中值\n  若叶节点与父节点空间已满，查找对应叶节点，插入目标 key，等半分裂；父节点插入两个新叶节点的中值，分裂为两个节点；将中值移入上级父节点，重复此步骤\nB+树将数据库分解成固定大小的块或者页面，传统上大小为 4KB，并且一次只能读取或写入一个页面。这种设计更接近于底层硬件，因为磁盘也被划分为固定大\n小的块。通常大多数数据库可以放入一个三到四层的 B+树，极大降低对页面的查找次数，从而大幅减少磁盘随机访问 I/O 次数。\n\n\n而对于 B 树，能够在非叶节点存储数据，可导致查询连续数据时产生更多的随机 I/O，而 B+树的所有叶节点通过指针连接，能够减少顺序遍历时产生的额外随机 I/O。\n\n\n优化\n\n\n  当然可以同时构建 B+树和哈希表的混合存储结构，来达到极致的性能，但也会带来更高的复杂度，在维护数据结构时，会导致更新和删除时需要操作更多份数据。\n  由于索引对只适用于直接访问（从左到右扫描匹配），因此通常能压缩索引；经过前缀压缩，可以有效提高 B+树扇出，降低树高，提高查找效率。\n  对于大量数据，重复迭代插入 B+树将会非常慢，此时，批量插入将会大幅提高效率。首先将所有数据进行排序，然后插入索引至叶节点首页或者尾页，最后重新\n分布节点。有效降低磁盘随机访问次数，优化并发控制。\n  所有叶节点自然排序，能够有效提高范围查找和邻近查找。\n\n\n多维索引\n\n无论是哈希索引还是有序索引都是一维索引，但是针对于类似地理空间信息等数据，亦或对于临近点、包含关系的查找等，一维索引有点难以应付，此时，便\n引入多维索引：建立在多维数据上的一种索引，支持有效多维查询，应用于部分匹配查找(Partial Match)、范围查找(Range)、最近临近点(Nearest neighbor)\n查找、位置查找(Where-am-I, Ponit)等。\n\n\n多维索引结构\n\n\n  Table-based\n    \n      Grid index files\n      Partitioned Hashing\n    \n  \n  Tree-like(Tree based)\n    \n      Multiple key indexes\n      kd-tree\n      Quad-tree\n      R-tree\n    \n  \n\n\nGrid Index file\n\n\n  构造成二维结构的索引\n\n\n\n\n\n  网格索引文件存储 m、n 大小的网格，存储网格桶(Buckets)，包含 mn 或(m+1)(n+1)个块指针\n  很容易扩展至高维索引\n  网格线有两种含义：\n    \n      网格线表示独立的点，即网格块存储对应键值（坐标值指向网格块）\n      网格线表示范围，网格块存储特范围键值（坐标值指向网格线）\n    \n  \n\n\n查找\n\n\n  查找横向索引（x）的位置\n  查找纵向索引（y）的位置\n  查找数据块指针偏移量\n    \n      offset = row index * (column index) + column index\n    \n  \n  在数据块中查找对应数据\n\n\n插入\n\n\n  定位待插入位置「bucket」\n  如果有空间则插入数据\n  若无，链接溢出块或者拆分 bucket\n\n\n多维查找情景\n\n\n  假设能够全部存储在内存中\n\n\n\n  无法表示对象，故不能支持 where-am-I queries\n  数据分布不均匀时，将产生许多空白空间\n  需要良好的算法支撑空间划分\n\n\n\n\nPartitioned Hashing\n\n通常使用的 Hashing 无法解决组合值「多维数据」问题；针对 n 个组合 key，Partitioned Hashing 使用 n 个哈希函数，每个函数对应一个子 key，\n哈希值即是这些单独哈希函数值的连接组合。\n\n\n多维查找情景\n\n\n  Partial Match queries\n匹配部分子 key，通常被用于减少搜索空间\n  Range queries\n通常不适用，因为哈希函数无法保存值的邻近性\n  Nearest neighbor queries\n不适用，由于哈希函数值是随机的，无法保存数据之间的真实距离\n  Where-am-I queries\n不适用，哈希函数不提供任何距离信息\n但是 Partitioned Hashing 能提供良好的分布率，相比 Grid index 使用更少的空间，可结合其他索引结构使用。\n\n\nMultiple-key index\n\n\n  多层级索引在不同维度使用不同类型（B-tree、Hashing）的索引\n\n\n\n  Partial Match queries\n搜索值在第一层索引时，是非常有效的。\n  Range queries\n查找部分或者全部有确定范围的数据。\n  Nearest neighbor queries\n扩展使用范围搜索「Range queries」查找最邻近点。\n  Where-am-I\n不适用\n\n\n\nkd-tree\n\n\n  kd-tree 是二叉搜索树（BST）的一种\n  在不同层级使用的搜索 key 属于不同的维度\n  不同层级上的维度会包围起来\n\n\n\n查找\n\n类似 BST\n\n插入\n\n类似 BST\n\n\n  Partial Match queries\n对于给定搜索值的维度，获取其中一个子树的值；反之，获取所有子树的值。\n  Range queries\n根据搜索范围获取对应子树的值\n  Nearest neighbor queries\n不适用\n  Where-am-I\n不适用\n\n\n\n\nQuad-tree\n\n一种每个维度对半划分的索引结构\n\n简化的 kd-tree，多维搜索类似 kd-tree\n\n\nR-tree\n\n从 B-tree 衍生的一种使用边界盒（bounding boxes）作为搜索 key 的索引结构。\n\n查找\n\n\n  从根节点开始\n  查找包含 key 的区域节点\n  若无，则不存在，查找结束\n  反之，递归查找所有访俄条件的区域子节点\n  到达叶节点时，查找到数据的位置\n\n\n插入\n\n\n  从根节点开始，尝试查找 key 适合的区域\n  若存在，往下重复查找；\n  若不存在，需要扩展已存在的区域\n    \n      尽量小的扩展\n    \n  \n  当到达叶节点，插入 key\n  若无空间，拆分区域节点\n    \n      类似 B-tree\n    \n  \n\n\n\n\n多维查找\n\n非常适用于 Partial Match queries, Range queries, Nearest neighbor queries, Where-am-I queries 四种情景，是多维索引一种高度理想结构。\n\n索引怎么使用？\n\n根据实际需求，选取不同的索引，达到空间与时间最优。通过了解索引的数据结构及算法，深入理解索引的优缺点及优化方式，比如最左前缀原理、索引长度、排序等。\n\n引用参考\n\n\n  https://github.com/Vonng/ddia/blob/master/en-us/ch3.md\n  https://en.wikipedia.org/wiki/Database_index\n  https://en.wikipedia.org/wiki/Hash_table\n  https://en.wikipedia.org/wiki/Search_engine_indexing\n  https://en.wikipedia.org/wiki/B%2B_tree\n  https://draveness.me/whys-the-design-mysql-b-plus-tree/\n  http://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/syl.html#CURRENT\n  https://web.cs.ucdavis.edu/~green/courses/ecs165b-s10/Lecture6.pdf\n  https://www.ibm.com/docs/en/db2woc?topic=objects-indexes\n  https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf\n  http://mlwiki.org/index.php/Indexing_(databases)\n  https://github.com/davidmoten/rtree\n  https://github.com/myui/btree4j\n  https://github.com/linpc2013/KDTree\n\n",
        "url": "/general/database/2022/04/20/RDMS-INDEX-TYPES/"
      },
    
      {
        "title": "工具资源",
        "excerpt": "一些常用、备用资源\n\n",
        "content": "一些常用、备用资源\n\n资源\n\n  \n    YYeTsBot\n  \n\n\n文章\n\n  \n    Java Memory Model\n  \n\n",
        "url": "/other/2022/06/28/%E5%B7%A5%E5%85%B7%E8%B5%84%E6%BA%90/"
      },
    
      {
        "title": "MySQL插入性能比较",
        "excerpt": "基于 MySQL 对数据库批量插入的性能研究\n\n",
        "content": "基于 MySQL 对数据库批量插入的性能研究\n\n概要\n\n\n  多条批量插入性能差，无论是否开启整体事务或重写批量语句\n  JDBC批量提交在开启重写批量语句时，性能得到极大提升\n  单条插入性能优良\n  Mybatis获取session插入提交「开启重写批量语句」与手写单条插入性能无异\n\n\n要点\n\n\n  JDBC原生性能最佳\n  存在批量插入的场景，开启rewriteBatchStatements\n\n\n批量插入模式\n\n数据库批量插入有几种模式：\n\n\n  多条插入(Multi single statements)\n  单条多值插入(Batch: Single statement with multi values)\n  批量插入(bulk insert provided by ORM)\n  批量插入(bulk insert provided by ORM with enable rewrite)\n\n\nJDBC\n\nSingle Inserts\n\n\n  Cost 360013ms\nCost 388987ms (enable rewriteBatchStatements)\n\n\npublic void handle(Connection connection) throws SQLException {\n    for (int i = 0; i &lt; ITERATION_COUNT; i++) {\n        final PreparedStatement statement = connection.prepareStatement(\"INSERT INTO t_perf (c1, c2, c3) VALUES (?, ?, ?)\");\n        statement.setInt(1, i);\n        statement.setFloat(2, i);\n        statement.setString(3, valueOf(i));\n        statement.execute();\n        statement.close();\n    }\n}\n\n\nBatch Insert\n\n\n  Cost 182008ms\nCost 869ms (enable rewriteBatchStatements)\n\n\npublic void handle(Connection connection) throws SQLException {\n    final PreparedStatement statement = connection.prepareStatement(\"INSERT INTO t_perf (c1, c2, c3) VALUES (?, ?, ?)\");\n    for (int i = 0; i &lt; ITERATION_COUNT; i++) {\n        statement.setInt(1, i);\n        statement.setFloat(2, i);\n        statement.setString(3, valueOf(i));\n        statement.addBatch();\n    }\n    log.info(\"{}\", statement);\n    statement.executeBatch();\n    statement.close();\n}\n\n\nDirty Bulk Insert\n\n\n  Cost 762ms\nCost 846ms (enable rewriteBatchStatements)\n\n\npublic void handle(Connection connection) throws SQLException {\n    final StringBuilder builder = new StringBuilder(\"INSERT INTO t_perf (c1, c2, c3) VALUES \");\n    for (int i = 0; i &lt; ITERATION_COUNT; i++) {\n        if (i != 0) {\n            builder.append(\",\");\n        }\n        builder.append(format(\"(%s, %s, '%s')\", i, i, i));\n    }\n    final String query = builder.toString();\n    final PreparedStatement statement = connection.prepareStatement(query);\n    log.info(\"{}\", statement);\n    statement.execute();\n    statement.close();\n}\n\n\nSafe Bulk Insert\n\n\n  Cost 716ms\nCost 808ms (enable rewriteBatchStatements)\n\n\nprivate String getInsertPlaceholders(int placeholderCount) {\n    final StringBuilder builder = new StringBuilder(\"(\");\n    for (int i = 0; i &lt; placeholderCount; i++) {\n        if (i != 0) {\n            builder.append(\",\");\n        }\n        builder.append(\"?\");\n    }\n    return builder.append(\")\").toString();\n}\n\n@SuppressWarnings(\"AssignmentToForLoopParameter\")\n@Override\npublic void handle(Connection connection) throws SQLException {\n    final int columnCount = 3;\n    final StringBuilder builder = new StringBuilder(\"INSERT INTO t_perf (c1, c2, c3) VALUES \");\n    final String placeholders = getInsertPlaceholders(columnCount);\n    for (int i = 0; i &lt; ITERATION_COUNT; i++) {\n        if (i != 0) {\n            builder.append(\",\");\n        }\n        builder.append(placeholders);\n    }\n    final int maxParameterIndex = ITERATION_COUNT * columnCount;\n    final String query = builder.toString();\n    final PreparedStatement statement = connection.prepareStatement(query);\n    int valueIndex = 0;\n    for (int parameterIndex = 1; parameterIndex &lt;= maxParameterIndex; valueIndex++) {\n        statement.setObject(parameterIndex++, valueIndex);\n        statement.setObject(parameterIndex++, valueIndex);\n        statement.setObject(parameterIndex++, valueIndex);\n    }\n    log.info(\"{}\", statement);\n    statement.execute();\n    statement.close();\n}\n\n\nSingle inserts in a transaction\n\n\n  Cost 402029ms\n\n\npublic void handle(Connection connection) throws SQLException {\n    connection.setAutoCommit(false);\n    for (int i = 0; i &lt; ITERATION_COUNT; i++) {\n        final PreparedStatement statement = connection.prepareStatement(\"INSERT INTO t_perf (c1, c2, c3) VALUES (?, ?, ?)\");\n        statement.setInt(1, i);\n        statement.setFloat(2, i);\n        statement.setString(3, valueOf(i));\n        statement.execute();\n        statement.close();\n    }\n    connection.commit();\n}\n\n\nORM(Mybatis)\n\nSingle inserts\n\n\n  Cost 41350ms\n\n\npublic void handle() {\n  for (Person person : persons) {\n    mapper.insert(person);\n  }\n}\n\n\nBatch insert\n\n\n  Cost 22141ms\nCost 1191ms (enable rewriteBatchStatements)\n\n\npublic void handle() {\n  PersonMapper mapper = sqlSession.getMapper(PersonMapper.class);\n  for (Person person : persons) {\n      mapper.insert(person);\n  }\n  sqlSession.commit();\n}\n\n\nBulk insert\n\n\n  Cost 1170ms\n\n\npublic void handle() {\n  personMapper.insertList(persons);\n}\n\n",
        "url": "/database/2022/09/05/Mysql%E6%8F%92%E5%85%A5%E6%80%A7%E8%83%BD/"
      },
    
      {
        "title": "CAP",
        "excerpt": "CAP: Consistency, Available, Partition\n\n",
        "content": "CAP: Consistency, Available, Partition\n\nConsistency patterns\nWeak Consistency\n\n  Real time\n  Lost data during connection loss\n\n\nEventual Consistency\n\n  Write =&gt; Read\n  Data is replicated asynchronously\n  In highly available systems\n\n\nStrong Consistency\n\n  Data is replicated synchronously\n  File system &amp; RDBMS\n  Transactions\n\n\nConsistency Model\nBackups\n\n  Make a copy\n  Sledgehammer\n  Weak consistency\n  Usually no transactions\n  Datastore: early internal launch\n\n\nMaster/Slave replication\n\n  Usually asynchronous\n  Good for throughput, latency\n  Most RDBMSes\n    \n      e.g. MySQL binary logs\n    \n  \n  Weak/eventual consistency\n    \n      Granularity matters!\n    \n  \n  Datastore: current\n\n\nMulti-master replication\n\n  Umbrella term for merging concurrent writes\n  Asynchronous, eventual consistency\n  Need serialization protocol\n    \n      e.g. timestamp oracle: monotonically increasing timestamps\n      Either SPOF with master election…\n      …or distributed consensus protocol\n    \n  \n  No global transactions!\n  Datastore: no strong consistency\n\n\nTwo Phase Commit\n\n  Semi-distributed consensus protocol\n    \n      deterministic coordinator\n    \n  \n  1: propose, 2: vote, 3: commit/abort\n  Heavyweight, synchronous, high latency\n  3PC buys async with extra round trip\n  Datastore: poor throughput\n\n\nPaxos\n\n  Fully distributed consensus protocol\n  “Either Paxos, or Paxos with cruft, or broken”\n    \n      Mike Burrows\n    \n  \n  Majority writes; survives minority failure\n  Protocol similar to 2PC/3PC\n    \n      Lighter, but still high latency\n    \n  \n\n\n\n\nAvailability Patterns\nFail-over\nActive-passive\nWith active-passive fail-over, heartbeats are sent between the active and the passive server on standby.\n\nActive-active\nBoth servers are managing traffic, spreading the load between them.\n\nReplication\nMaster-slave and master-master\n\nReferences\n\n  https://www.youtube.com/watch?v=srOgpXECblk\n\n",
        "url": "/distributed%20system/2023/06/05/CAP/"
      },
    
      {
        "title": "Prompts for ChatGPT",
        "excerpt": "Best practice prompts for chatGPT\n\n",
        "content": "Best practice prompts for chatGPT\n\nStrategy: Write clear instructions\n\nInclude details in your query to get more relevant answers\n\nIn order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.\n\nAsk the model to adopt a persona\n\nThe system message can be used to specify the persona used by the model in its replies.\n\nUse delimiters to clearly indicate distinct parts of the input\n\nDelimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.\n\nSpecify the steps required to complete a task\n\nSome tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.\n\nProvide examples\n\nProviding general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intended for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as “few-shot” prompting.\n\nSpecify the desired length of the output\n\nYou can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.\n\nProvide reference text\n\nInstruct the model to answer using a reference text\n\nIf we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.\n\nInstruct the model  to answer with citations from a reference text\n\nIf the input has been supplemented with relevant knowledge, it’s straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\n\nSplit complex tasks into simpler subtasks\n\nUse intent classification to identify the most relevant instructions for a user query\n\nFor tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages.\n\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\n\nSummarize long documents piecewise and construct a full summary recursively\n\nGive GPTs time to “think”\n\nInstruct the model to work out its own solution before rushing to a conclusion\n\nSometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion.\n\nUse inner monologue or a sequence of queries to hide the model’s reasoning process\n\nInner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.\n\nAsk the model if it missed anything on previous passes\n\nUse externals tools\n\nCompensate for the weaknesses of GPTs by feeding them the outputs of other tools.\n\nUse embeddings-based search to implement efficient knowledge retrieval\n\nA model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.\n\nUse code execution to perform more accurate calculations or call external APIs\n\nTest changes systematically\n\nEvaluate model outputs with reference to gold-standard answers\n\nSuppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.\n",
        "url": "/skills/2023/06/06/Prompts-for-ChatGPT/"
      },
    
      {
        "title": "一致性校验",
        "excerpt": "分布式系统数据数据一致性\n\n",
        "content": "分布式系统数据数据一致性\n\n业务侧系统保证最终一致性\n\n核心思想\n\n通过业务系统的服务保证数据的最终一致性，业务系统侧系统记录每一次具体业务操作的执行流水日志信息，并且对没有全部成功的变更结果，触发执行数据一致性的校验核对。\n\n设计原则\n\n\n  根据业务操作标识和业务操作唯一ID实现接口的幂等设计\n  实时的同步检查核对、准实时的异步检查核对、定时任务的异步检查核对\n\n\n流程图\n\n\n  \n    同步执行\n\n  \n  \n    异步核对\n\n  \n\n\n平台侧系统保证最终一致性\n\n核心思想\n\n平台侧系统的每一次数据变更，主动寻找业务侧系统，确认本次数据变更结果\n\n设计的基本原则\n\n\n  根据业务操作标识和唯一ID做幂等设计\n\n\n流程图\n\n\n  \n    同步核对\n\n  \n  \n    异步核对\n\n  \n\n",
        "url": "/application/2023/06/06/%E4%B8%80%E8%87%B4%E6%80%A7%E6%A0%A1%E9%AA%8C/"
      },
    
      {
        "title": "架构设计",
        "excerpt": "一些架构设计要点\n\n",
        "content": "一些架构设计要点\n\n\n\n收益非技术本身\n\n\n  降低技术门槛，加快开发效率\n  提高系统稳定性\n  简化、自动化，降低成本\n\n\n以应用服务和API为视角，不以资源、技术为视角\n\n选择最主流、成熟的技术\n\n\n  成熟、工业化的技术栈\n  全球流行的技术\n  红利大的主流技术\n\n\n完备性 &gt; 性能\n\n\n  最科学、严谨的技术模型\n\n\n制定病遵循服从标准、规范和最佳规范\n\n\n  服务间调用的协议标准、规范\n  命名的标准和规范\n  日志和监控的规范\n  配置上的规范\n  中间件使用的规范\n  软件、开发库版本统一\n\n\n重视架构扩展性、可运维性\n\n\n  服务编排-&gt;降低服务间耦合\n    \n      Workflow\n      Event Driven\n      Broker\n      Gateway\n      Service Discovery\n    \n  \n  服务发现、服务网关，降低运维复杂度\n  设计原则\n\n\n收口控制逻辑\n\n\n  流量\n  服务治理\n  监控数据\n  资源调度\n  中间件\n\n\nReferences\n\n\n  https://coolshell.cn/articles/21672.html\n\n",
        "url": "/%E9%9A%8F%E7%AC%94/2023/06/27/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"
      },
    
      {
        "title": "阅读论文的方法",
        "excerpt": "阅读论文的方法\n\n",
        "content": "阅读论文的方法\n\n日常需要通过阅读各种论文来了解最新的、前沿的知识与技术，或者研读经典，来增强视野与技能。\n\n本文主要翻译总结一些阅读论文的方法论文章。\n\nHow to Read an Academic Article\n\n\n  翻译文章: https://organizationsandmarkets.com/2010/08/31/how-to-read-an-academic-article/\n\n\n浏览、阅读、处理的基本步骤\n\n\n  阅读摘要「Abstract」\n  阅读介绍「Introduction」\n  阅读结论「Conclusion」\n  快读中间部分，查看章节标题、表格、图表，尝试了解文章的风格、流程。\n    \n      论文属于方法论、概念性、理论性、经验性，或是其他\n      主要是一项调查、新颖的理论贡献、现有理论或技术的实证应用、评论，或是其他\n    \n  \n  返回快速阅读整个内容，跳过方程式、大多数表格和图表\n  返回并仔细阅读整篇论文，重点关注看起来最重要的部分或领域\n\n\n基于基本论点，提出评论\n\n\n  询问这个论证是否合理、内在是否一致、是否有充分的论据或证据支持\n  与读过的相同或密切相关主题的其他论文进行比较，论点是否一致、矛盾、不相关\n  使用Google Scholar等资源查找引用此论文的论文、博客、群组，查看其他人的评论\n  查看参考文献，了解本文如何适应其主体领域的更广泛背景\n\n\nHow to Read a Paper\n\n\n  翻译论文: http://ccr.sigcomm.org/online/files/p83-keshavA.pdf\n\n\n三遍法\n\n最关键的是，应该最多读三遍论文，而不是从头开始，一直到最后。每一次都需要建立在上一次的基础上，完成特定的目标：\n\t1. 第一遍：对论文有总体的了解\n\t2. 第二遍：掌握论文的内容\n\t3. 第三遍：深入理解论文\n\n第一遍\n\n快速浏览文章，决定需要重点阅读的部分：\n\n  仔细阅读标题、摘要、引言\n  阅读章节、子章节标题，忽略其他内容\n  阅读结论\n  大致浏览参考文献，标记已经阅读过的文献\n\n\n五个问题：\n\n  类别：这是篇论文的类型–测量类、现有系统的分析、研究原型的描述\n  背景：相关的其他论文，分析这个问题的理论基础\n  正确性：文中假设的有效性\n  贡献：论文的主要贡献\n  清晰度：论文的卓越度、亮点\n\n\n根据以上的结论，决定是否进一步阅读。\n\n第二遍\n\n更加仔细地阅读，忽略证明之类的细节，并且记录要点，在空白处标记评论\n\n\n  仔细查看图形、图表、其他解释。特别注意图表，如坐标轴标记是否合适，是否有误差标识，显著显示结论。\n  标记相关的未读参考论文，延伸阅读\n\n\n第二遍需要一个小时左右，能够掌握论文的内容。能够向其他人总结论文的主旨和支撑论据。这种详细程度适合感兴趣的的非本专业论文。\n如果由于新的、陌生的主题，不熟悉的术语、首字母缩略词，不理解的证明、实验技术，导致难以理解大部分内容。可能是因为论文本身不好，或者有未经证实的断言、大量的前向参考。有三个选择：\n\n  把论文扔开\n  阅读背景材料后，再次理解\n  坚持第三遍\n\n\n第三遍\n\n要完全理解一篇论文，需要第三遍。第三遍的重点的关键是尝试虚构复现论文：即做出与作者相同的假设，重新创作作品。通过这种重新创作，与实际论文比较，可以轻松识别论文的创新之处，并识别隐藏的缺陷和假设。\n\n需要识别并挑战陈述中的每一个假设，试想用自己的方法表述。根据记忆重建论文的整个结构，并能识别其优点、缺点，辨识其中的隐含的假设、引用缺失以及实验或分析技术的潜在问题。\n\n文献查阅\n\n使用学术搜索引擎和关键词查找该领域的三到五篇最新论文。再次，访问该领域的关键论文和研究人员的网站，查看最近发表的文章。最后，访问这些顶级会议的网站，查看最近的会议记录。\n",
        "url": "/reading/2023/06/28/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E7%9A%84%E6%96%B9%E6%B3%95/"
      },
    
      {
        "title": "How does Stable Diffusion work",
        "excerpt": "本文翻译 https://stable-diffusion-art.com/how-stable-diffusion-work/\n\n",
        "content": "本文翻译 https://stable-diffusion-art.com/how-stable-diffusion-work/\n\nStable Diffusion: 稳定扩散\n\nStable Diffusion是一种深度学习模型。\n\n稳定扩散能做什么\n\n最简单的形式，稳定扩散是一种文本到图像模型（text-to-image）。给它一个文字提示（text prompt），将返回与文本匹配的图像。\n\n\n扩散模型\n\nStable Diffusion属于一类称为扩散模型（diffusion models）的深度学习模型，是旨在生成与训练中类似数据的生成模型。在稳定Stable Diffusion的情况下，数据是图像。\n\n为什么称为扩散模型？是由于它的数学看起来像物理学中的扩散。\n\n假设只用两张图训练了一个扩散模型：猫和狗。在下图中，左边的两个峰代表猫和狗图像组。\n\n前向扩散\n\n\n\n前向扩散（Forward diffusion）过程中，向训练图像添加噪声，逐渐将其变成无特征的噪声图像。最终，将无法分辨图片最初是狗或者猫。\n\n就像一滴墨水落入一杯水中，墨滴在水中扩散，几分钟后，随机分布在整个水中，无法判断最初是落在中心还是边缘附近。\n\n\n\n反向扩散\n\n如何逆转扩散？像倒放视频一样，倒退时间，将会看见墨滴最初滴落的位置。\n\n\n\n从技术上讲，每个扩散都有两个部分：（1）漂移（drift）、（2）随机运动（random）。反向扩散图像会偏向猫或狗，而不会介于两者中间。\n\n训练是如何进行的\n\n反向扩散的想法是巧妙而优雅的。\n\n为了反转扩散，需要知道图像中添加了多少噪声——通过训练神经网络模型来预测添加的噪声。在Stable Diffusion中，被称为噪声预测器——一个U-Net模型。训练过程如下：\n\n\n  选择一张训练图像，比如猫。\n  生成随机噪声图像。\n  通过一定数量的步骤，添加噪声图片来破坏训练的图像。\n  教噪声训练器反馈添加的噪声数量——通过调整权重并且向他展示正确答案。\n\n\n\n\n训练后，具备一个能够估计添加到图像中的噪声的噪声预测器。\n\nReverse diffusion\n\n如何使用噪声预测器？\n\n首先生成一个完全随机的图像，让噪声预测器告知噪声。然后从原始图像中减去估计的噪声，重复此过程几次，将获得猫或狗的图像。\n\n\n\n但是目前无法控制生成猫或狗的图像，图像生成是无条件的。\n\n有关反向扩散采样和采样器的更多信息。\n\n稳定扩散模型\n\n上述扩散过程是在图像空间中进行的，不是稳定扩散的工作原理，无法在单个GPU上运行。\n\n图像空间是巨大的。试想：三个颜色通道（红、蓝、绿）的512×512图像具有786432维的空间。\n\n类似Imagen和DALL-E的扩散模型都在像素空间中，虽然使用了一些技巧来加速模型，但是仍然不够。\n\n潜在扩散模型（Latent diffusion model）\n\nStable Diffusion旨在解决速度问题。\n\nStable Diffusion是一种潜在扩散模型。它不是在高维图像空间中操作，而是首先将图像压缩到潜在空间中。潜在空间小了48倍，因此降低了处理次数——变快的原因。\n\n变分自动编码器（Variational Autoencoder）\n\n上述过程是通过一种变分自动编码器的技术实现。\n\n变分自动编码器神经网络有两部分：（1）编码器、（2）解码器。编码器将图像压缩为潜在空间中的低维表示，解码器从潜在空间中恢复图像。\n\n\n\n稳定扩散模型的潜在空间是4×64×64，比图像像素空间小48倍。所有前向和反向扩散实际上都是在潜在空间中完成的。\n\n因此，在训练过程中，不会生成噪声图像，而是在潜在空间（潜在噪声）中生成随机张量。不是用噪声破坏图像，而是用潜在噪声破坏图像在潜在空间中的表示。由于潜在空间小，所以速度快很多。\n\n图像分辨率（Image resolution）\n\n图像分辨率反映在潜在图像张量上，512×512图像的潜在图像大小仅为4×64×6，768×512肖像图的潜在图像为4×96×96。因此需要更长、更多的VRAM才能生成更大的图像。\n\n由于Stable Diffusion v1是在512×512图像上进行了微调（fine-tuned），因此生成大于512×512的图像可能会得到重复的图像，比如两个头。如果是必须的，至少在一侧保留512像素，并用AI升级器获得更高的分辨率。\n\n为什么潜在空间是可能的（Why is latent space possible）\n\n为什么VAE可以将图像压缩到更小的潜在空间而不丢失信息——原因是自然图像不是随机的，它们具有高度的规律性：面部遵循眼睛、鼻子、脸颊和嘴巴之间的特定空间关系。换而言之，图像的高维性是认为的。自然图像可以很容易地压缩到更小的潜在空间中，而不会丢失任何信息——在机器学习中被称为流行假设（manifold hypothesis）。\n\n潜在空间中的反向扩散（Reverse diffusion in latent space）\n\n稳定扩散中潜在反向扩散的工作原理：\n\n\n  生成随机潜在空间矩阵。\n  噪声预测器估算潜在矩阵的噪声。\n  从潜在矩阵中减去估算的噪声。\n  重复步骤2&amp;3直至达到特定的采样步骤。\n  VAE解码器将潜在矩阵转换为最终图像。\n\n\nVAE文件是什么（What is a VAE file）\n\n在Stable Diffusion v1中，VAE文件被用来改善眼睛和面部，即前面提到的自动编码器的解码器。通过进一步微调解码器，模型可以绘制更精细的细节。\n\n将图像压缩到潜在空间会丢失信息，是因为原始VAE无法恢复精细细节，但是，VAE解码器能够负责绘制精细的细节。\n\n调理（Conditioning）\n\n目前我们的理解尚不完整：文字提示如何进入图片？没有文本，stable diffusion就不是text-to-image 模型，将会得到无法控制的猫或狗的图像。\n\n所以需要用到调理——引导噪声预测器，以便在从图像中减去预测的噪声后，能够提供预想的图像。\n\n文本调节（Text conditioning「text-to-image」）\n\nTokenizer首先将提示中的每个单词转换为称为词元（token）的数字，然后每个token被转换为768个值的向量——嵌入（embedding）。这些embeddings随后由文本转换器（text transformer）进行处理，提供给噪声预测器使用。\n\n\n\n分词器（Tokenizer）\n\n\n\n词元化（tokenization）是计算机理解单词的方式。人类可以读取文字，但是计算机只能读取数字，所以需要将文本提示中的单词转换为数字。\n\n分词器只能对训练期间的单词进行分词。例如，CLIP模型中有“dream”和“beach”，但没有“dreambeanch”。分词器会将“dreambeach”一词分解为两个词元，“dream”和“beach”，所以一个词并不总意味一个词元。\n\n另外空格字符也是词元的一部分。上述例子中，“dream beach”和“dreambeach”生成的词元不同。\n\nStable Diffusion模型仅限于在提示中使用75个词元。\n\n嵌入（Embedding）\n\n\n\nStable diffusion v1使用Open AI的ViT-L/14 Clip模型，其中Embedding是一个768个值的向量，每一个词元都有自己独特的嵌入向量。Embedding由在训练过程中学习的CLIP模型固定。\n\n为什么需要embedding？因为有些词彼此之间密切相关，而我们想要利用这些信息。例如，man、gentleman、guy的嵌入几乎相同，因为它们可以互相转换使用。Monet、Manet、Degas都是用不同的方式以印象派风格作画，这些名称具有接近但不相同的嵌入。\n\n这与使用关键词触发的嵌入相同，嵌入可以发挥魔法。科学家已经证实找到正确的嵌入可以触发任意对象和样式——一种称为文本反转（textual inversion）的微调技术。\n\n将嵌入提供给噪声预测器（Feeding embeddings to noise predictor）\n\n\n\n在输入噪声预测器之前，嵌入需要由文本转换器（text transformer）进一步处理。transformer像一个通用适配器，输入文本嵌入向量，也可以像类标签、图像、深度图之类的其它东西。transformer不仅进一步处理数据，也提供提供一种包含不同调节模式/训练方式（conditioning modalities）的机制。\n\n交叉注意力（Cross-attention）\n\n整个U-Net中的噪声预测器多次使用文本转换器的输出，U-Net通过交叉注意力机制来消耗它，即提示（prompt）和图像（image）的结合处。\n\n以提示“A man with blue eyes”为例，stable diffusion将“blue”、“eyes”两个词配对在一起（提示中的自我关注），这样即会生成一个蓝眼睛的男人，而不是一个穿蓝色衬衫的男人。然后，使用此信息引导反向扩散至包含蓝眼睛的图像（提示和图像之间的交叉注意力）。\n\n旁注：超网络（Hypernetwork），一种调优稳定扩散模型的技术，通过劫持交叉注意力网络来插入样式。LoRA models修改交叉注意力模块的权重来改变风格，单独修改这个模块就可以调优Stable Diffusion模型。\n\n其他条件（Other conditionings）\n\n文本提示并不是调节Stable Diffusion模型的唯一方法。文本提示（text prompt）和深度图像（depth image）都用于调节深度到图像（depth-to-image）模型。\n\nControlNet使用检测到的轮廓、人体姿势等来调节噪声预测器，并实现对图像生成的出色控制。\n\n逐步稳定扩散（Stable Diffusion step-by-step）\n\n文本转图像（Text-to-image）\n\n在文本转图像中，给stable diffusion提供文本提示，将返回图像。\n\nStep 1：Stable Diffusion在潜在空间中生成随机张量，可以通过设置随机数生成器种子在控制该张量。如果将种子设置为某个值，将始终获得相同的随机张量——即潜在空间中的图像，目前全是噪音。\n\n\n\nStep 2：噪声预测器U-Net将潜在噪声图像和文本提示作为输入，并在潜在空间（一个4×64×64张量）中预测噪声。\n\n\n\nStep 3：从潜在空间中减去潜在噪声，得到新的潜像。\n\n\n\n重复步骤2&amp;步骤3一定数量的采样步骤。\n\nStep 4：最后，VAE的解码器将潜在图像转换回像素空间，得到运行稳定扩散后的图像。\n\n\n\n下面是图像在每个采样步骤中的演变方式。\n\n\n\n噪音表（Noise schedule）\n\n图像从模糊变得清晰（noisy to clean），试图得到每个采样步骤中获得预期的噪声——噪声表（noise schedule）。\n\n\n\n噪声序列表是我们定义的，可以选择在每一步减去相同数量的噪声，或者在开始时减去更多。采样器在每一步中减去足够的噪声，以在下一步中达到预期噪声。\n\n图像到图像（Image-to-image）\n\nImage-to-image是SDEdit方法中首次提出的方法，可应用于任何扩散模型。因此，我们有图像到图像的稳定扩散（潜在扩散模型）。\n\n输入图像和文本提示被输入到image-to-image，生成的图像将受到输入图像和文本提示的限制。例如，使用这张业余绘画和提示“photo of perfect green apple with stem, water droplets, dramatic lighting”作为输入，图像到图像可以将其变成专业绘画。\n\n\n\n下面是分布过程：\n\nStep 1：输入图像被编码到潜在空间。\n\n\n\nStep 2：将噪声添加到潜像中，去噪强度（denoising strength）控制添加的噪声量。如果为0，不添加噪声；如果为1，添加最大量的噪声，使潜像称为完全随机的张量。\n\n\n\nStep 3：噪声预测器U-Net将潜在噪声图像和文本提示作为输入，并预测潜在空间（4×64×64张量）中的噪声。\n\n\n\nStep 4：从潜在图像中减去潜在噪声，变成新的潜像。\n\n\n\n重复步骤3&amp;步骤4至采样步骤的一定数量。\n\nStep 5：最后，VAE的解码器将潜像转换回像素空间，得到运行image-to-image后的图像。\n\n\n\n所以，图像到图像——设置带有一点噪声和一点输入图像的初始潜在图像，设置去噪强度为1相当于文本转图像，因为初始潜像完全是随机噪声。\n\n修复\n\n修复只是图像到图像的一种特殊情况，噪声会被添加到想要修复的图像部分，噪声量同样有降噪强度控制。\n\n深度到图像（depth-to-image）\n\n深度到图像是图像到图像的增强，使用深度图（depth map）生成带有附加条件的新图像。\n\nStep 1：输入图像被编码为潜在状态。\n\n\n\nStep 2：MiDaS（一种AI深度模型）根据输入图像估算深度图。\n\n\n\nStep 3：将噪声添加到潜像中，去噪强度控制被添加的噪声量。如果去噪强度为0，不添加噪声；去噪强度为1，添加最大噪声，使得潜像变成随机张量。\n\n\n\nStep 4：噪声预测器根据文本提示和深度图估算潜在空间的噪声。\n\n\n\nStep 5：从潜像中减去潜在噪声，得到新的潜像。\n\n\n\n重复采样步骤数的步骤4&amp;5。\n\nStep 6：VAE的解码器对潜像进行解码，将获得从深度到图像的最终图像。\n\n\n\nCFG值是什么（What is CFG value）\n\n无分类器指导（Classifier-Free Guidance）（CFG），前身：分类指导（classifier guidance）。\n\n分类器指导（classifier guidance）\n\n分类器指导是一种将图像标签（image labels）合并到扩散模型中的方法，可以使用标签来指导扩散过程。例如，标签“cat”引导反向扩散过程生成猫的照片。\n\n分类器指导尺度（classifier guidance scale）是控制扩散过程遵循标签程度的参数。\n\n例如，假设有3组图像，标签为“cat”、“dog”、“human”。若扩散是无引导的，模型将从每组总的群体抽取样本，但是有时候可能会抽取适合两个标签的图像，比如一个男孩在抚摸一只狗。\n\n\n\n在高分类器指导下，扩散模型生成的图像将偏向极端或明确的示例。如果向模型寻求一只猫，它会返回一张明确是猫的图像，除此之外别无他法。\n\n分类器指导尺度控制指导的遵循程度。上图中，右边的采样比中间的采样具有更高的分类器指导尺度。实际上，该比例值只是带有该标签数据的漂移项的乘数。\n\n无分类器指导（classifier-free guidance）\n\n尽管分类器指导实现了破纪录的性能，但它需要一个额外的模型来提供该指导，这给训练带来了一些困难。\n\n无分类指导（classifier-free guidance）是一种实现“没有分类器的分类器指导”的方法，没有使用类标签和单独的模型进行指导，而是使用图像标题训练条件扩散模型（conditional diffusion model），就像上述文本到图像的模型一样。\n\n将分类器部分作为调节噪声预测器U-Net，实现图像生成中的“无分类器”指导。\n\nCFG值（CFG value）\n\n通过调节获得了一个无分类器扩散过程，如何控制遵循指导的量？\n\n无分类器指导比例是一个控制文本提示对扩散过程影响程度的值，当值为0时，图像生成是无条件的（无提示），较高的值将引导扩散至提示。\n\nStable Diffusion v1 vs v2\n\n模型差异（Model difference）\n\nStable Diffusion v2使用OpenClip进行文本嵌入，Stable Diffusion使用Open AI的CLIP ViT-L/14进行文本嵌入：\n\n\n  OpenClip扩大了5倍更大的文本编码器模型可以提高图像质量。\n  尽管Open AI的CLIP模型是开源的，但是这些模型使用专有数据进行训练。切换到OpenClip模型使研究和优化模型更透明，利于长远发展。\n\n\n训练数据差异（Training data difference）\n\nStable Diffusion v1.4训练：\n\n\n  laion2B-en数据集上以256×256的分辨率进行237k步（237k steps at resolution 256×256 on laion2B-en dataset.）\n  laion-high-resolution上以512×512分辨率进行194k步（194k steps at resolution 512×512 on laion-high-resolution.）\n  文本调节降低10%，laion-aesthetics v2 5+上以512×512进行225k步（225k steps at 512×512 on “laion-aesthetics v2 5+“, with 10% dropping of text conditioning.）\n\n\nStable Diffusion v2训练：\n\n\n  550k steps at the resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score &gt;= 4.5.\n  850k steps at the resolution 512x512 on the same dataset on images with resolution &gt;= 512x512.\n  150k steps using a v-objective on the same dataset.\n  Resumed for another 140k steps on 768x768 images.\n\n\nStable Diffusion v2.1在v2.0上进行了微调：\n\n\n  additional 55k steps on the same dataset (with punsafe=0.1)\n  another 155k extra steps with punsafe=0.98\n\n\n所以基本上，都在最后的训练步骤中关闭了NSFW过滤器。\n\n结果差异（Outcome difference）\n\n用户通常发现使用Stable Diffusion v2来控制风格和生成名人更困难，尽管Stability AI没有明确过滤艺术家和名人的名字，但是它们的效果在v2中很弱，可能是由于训练数据的差异造成的。Open AI的专有数据可能有更多艺术品和名人照片，数据可经过贵都过滤，因此看起来都更好。\n",
        "url": "/reading/2023/06/30/How-does-Stable-Diffusion-work/"
      },
    
      {
        "title": "What Is a Transformer Model",
        "excerpt": "本文翻译自：\n\n  https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n  http://jalammar.github.io/illustrated-transformer/\n\n\n",
        "content": "本文翻译自：\n\n  https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/\n  http://jalammar.github.io/illustrated-transformer/\n\n\nTransformer模型是什么？\n\nTransformer模型是一种神经网络，通过跟踪连续数据中的关系来学习上下文并从理解含义。\n\nTransformer模型应用一组称为注意或者自注意（attention or self-attention）的数学技术，来检测疏远数据元素（distant data elements）之间不明显的一系列相互影响、依赖的方式。\n\nGoogle 2017年的一篇论文首次描述了Transformer，它是迄今为止被发明的模型中最新、最强大的类别之一。推动着机器学习领域的进步，有些被称为“变形人工智能”（transformer AI）。\n\n斯坦福大学的研究人员在2021年8月的一篇论文中将Transformer称为“基础模型”（foundation models）。\n\nTransformer模型能做什么？\n\nTransformer可以近实时地翻译文本和语音，向多元化和听力障碍的与会人员开放会议和课堂。\n\n帮助研究人员了解DNA中的基因链和蛋白质中的氨基酸，从而加快药物设计。\n\n\n\n高层级视角（A High-Level Look）\n\n首先将该模型视为一个黑盒，在机器翻译应用程序中，输入一种语言，返回另一种语言。\n\n\n\n解剖中间，可以得到一个编码组件、一个解码组件以及它们间的链接。\n\n\n\n编码组件是一堆编码器，解码组件是相同数量解码器的堆栈。\n\n\n\n这些解码器的结构是相同的（不共享权重），每一层分为两个子层：\n\n\n\n编码器的输入首先流经自注意层——帮助编码器在对特定单词编码时查看输入句子中的其他单词。自注意层的输出被馈送到前馈神经网络（feed-forward neural network），独立应用完全相同的前馈神经网络到每个位置。\n\n解码器具有这两层，但中间有一个帮助解码器关注输入句子的相关部分的注意层。\n\n\n\n张量代入图片（Bringing The Tensors Into The Picture）\n\n接下来了解各种向量/张量以及他们如何在这些组件之间流动，将训练模型的输入转换为输出。\n\n与NLP应用中的一般情况一样，首先使用嵌入算法（embedding algorithm）将每个单词转换为向量。\n\n\n\n嵌入（embedding）仅发生在最底部的编码器中。所有编码器共有的抽象是，接收每个大小为512的向量列表——在底部编码器中，会是单词嵌入，但在其他编码器中，将是下面编码器的输出。列表的大小可以设置——基本上是训练数据集中最长句子的长度。将单词嵌入到输入序列后，每个单词都回流经编码器的两层。\n\n\n\n这里有Transformer的一个关键属性，即每个位置的单词在编码器中都流经自有路径。自注意层在这些路径中存在依赖关系。然而，前馈层不具有这些依赖性，因此各种路径可以在流经前馈层时并行处理。\n\n编码（Encoding）\n\n编码器接收向量列表作为输入，通过将这些向量传递到“自注意”层（self-attention layer），然后传递到前馈神经网络，将输出向上发送到下一个编码器来处理。\n\n\n\n高层次自注意（Self-attention at a High Level）\n\n当模型处理每个单词（输入序列中的每个位置）时，自注意允许它查看输入序列中的其他位置来寻找有助于更好地编码该单词的线索。\n\n自注意是Transformer用来将其他相关单词的“理解”融入正在处理的单词中的方法。\n\n\n当在编码器#5（堆栈中的顶部编码器）中对单词“it”进行编码时，注意机制的一部分集中在“The Animal”上，并将其表示的一部分聚集到“it”的编码中\n\n自注意细节（Self-Attention in Detail）\n\n下文分析如何使用向量计算自注意，与如何使用矩阵实现。\n\n计算自注意力的第一步是从每个编码器的输入向量创建三个向量，因此，对于每个单词，创建一个查询向量（Query vector）、一个键向量（Key vector）、一个值向量（Value vector）。通过将嵌入（embedding）乘以在训练过程中训练的三个矩阵创建这些向量。这些新向量的维度小于嵌入向量——维度为64，而嵌入与编码器的输入、输出向量的维度为512——这是一种架构选择，可以使多头注意力的计算（大部分）保持恒定。\n\n\n将x1乘以WQ权重矩阵得到q1，即与该单词关联的“查询”向量。最终为输入句子中的每个单词创建一个“查询”、“键”、“值”投影\n\n第二步是计算分数，假设计算单词“Thinking”的自注意力，需要根据输入句子的每个单词对这个单词进行评分。当在某个位置对单词进行编码时，分数决定了对输入句子的其他部分的关注程度。分数是通过计算查询向量（query vector）与需要评分的单词的键向量（key vector）的点积（dot product）得到。因此，如果计算处理位置#1中单词的自注意力，第一个分数将是q1和k1的点积，第二个分数是q1和k2的点积。\n\n\n\n第三步和第四步将这些分数除以8（键向量维度的平方根是64（默认值），使梯度更稳定，也可以是其他值），然后将结果传递给softmax运算（softmax operation）。Softmax对分数进行归一化，使它们全部为正数并且总和为1。\n\n\n\nsoftmax分数决定了每个单词在这个位置上的表述量。显然，单词所在位置具有最高的softmax分数，但有时关注与当前单词相关的另一个单词是有用的。\n\n第五步，将每个值向量（value vector）乘以softmax分数，很直观地保持关注单词的完整性，并覆盖不相关的单词（比如将它们乘以0.001这样的小数字）。\n\n第六步，对加权值向量（weighted value）求和，产生该位置自注意力层的输出。\n\n\n\n自注意力层计算得到的向量可以发送到前馈神经网络。在实际中，可以通过矩阵形式计算，更快的处理。\n\n自注意的矩阵计算（Matrix Calculation of Self-Attention）\n\n第一步，计算查询、键、值矩阵。将嵌入（embedding）打包到矩阵X中，并乘以已训练过的权重矩阵（WQ、WK、WV）。\n\n\nX矩阵中的每一行对应于输入句子中的一个单词\n\n最后，由于处理的是矩阵，可以将第二步到第六步压缩为一个公式来计算自注意层的输出。\n\n\n矩阵形式的self-attention计算\n\nThe Beast With Many Heads\n\n论文通过“多头”注意力机制进一步细化自注意层，通过下面两种方式提高注意层的性能：\n\n\n  扩展模型关注不同位置的能力。z1包含一些其他编码，但它可能由实际单词本身主导。\n  为注意层提供了多个“表示子空间”（representation subspaces）。通过多头注意，不只拥有一组查询/键/值权重矩阵，而是拥有多组查询/键/值权重矩阵（Transformer使用八个注意头，因此最终为每个编码器/解码器提供八组）。这些集合中的每一个都是随机初始化的。然后，在训练之后，每个集合用于将输入嵌入（或来自较低编码器/解码器的向量）投影到不同的表示子空间中。\n\n\n\n通过多头注意，为每个头维护单独的Q/K/V权重矩阵，从而产生不同的Q/K/V矩阵\n\n如果进行上图相同的自注意计算，只需用不同的权重矩阵进行八次不同的计算，最终得到八个不同的Z矩阵。\n\n\n\n而前馈层不需要八个矩阵——只需要一个矩阵（每个单词一个向量），所以需要一种方法将这八个压缩乘一个矩阵。将矩阵连接起来，然后乘以一个附加权重矩阵WO。\n\n\n\n这几乎便是多头注意的全部内容。\n\n\n\n对示例句子的“it”进行编码时，不同注意头聚焦的位置。\n\n\n对“it”进行编码时，一个注意头主要关注“animal”，而另一个注意头关注“tired”。从某种意义上，模型对“it”的表征会包含一些“animal”和“tired”的表征\n\n如果将所有注意头添加到图片中，将会更难解释：\n\n\n\n使用位置编码表示序列的顺序（Representing The Order of The Sequence Using Positional Encoding）\n\n到目前为止，模型缺少一种解释输入序列中单词顺序的方法。为解决这个问题，transformer向每个输入嵌入添加一个向量。这些向量遵循模型学习的特定模式，有助于确认每个单词的位置，或序列中不同单词之间的距离。\n\n在嵌入向量投影到Q/K/V向量和点积注意时，将这些值添加到嵌入向量中，就可以提供有意义的距离。\n\n\n为了让模型理解单词的顺序，添加位置编码向量——值遵循特定的模式\n\n假设嵌入维数为4，则实际的位置编码如下：\n\n\n\n下图中，每一行对应一个向量的位置编码。因此，第一行将是输入序列中添加到第一个单词嵌入的向量。每一行包含512个值，每个值介入1和-1之间。\n\n\n嵌入大小为512（列）的20个单词（行）的位置编码。左半部分的值由正弦函数生成，右半部分由余弦函数生成，连接起来行程每个位置编码向量\n\n残差（The Residuals）\n\n每个编码器中的每个子层（自注意，ffnn）周围都有一个残差连接，并且后面是层归一化步骤。\n\n\n\n可视化自注意相关的向量和层范数操作（layer-norm operation）：\n\n\n\n同样适用于解码器的子层——一个由2个堆叠编码器和解码器组成的Transformer：\n\n\n\n解码器端（The Decoder Side）\n\n编码器首先处理输入序列，然后顶部编码器的输出被转换为一组注意向量K和V。每个解码器在其“编码器-解码器注意（encoder-decoder attention）”使用这些向量，有助于解码器关注输入序列中的适当位置。\n\n\n\n完成编码阶段后，开始解码阶段。解码阶段的每个步骤都会输出输出序列中的一个元素\n\n接下来重复这一过程，直到出现一个特殊符号，表明transfomer解码器已完成输出。每个步骤的输出都会在下一时间点的步骤馈送到底部解码器，解码器会像编码器一样向上传递结果。\n\n像对编码器输入所做的那样，在解码器输入中嵌入并添加位置编码，以指示每个单词的位置。\n\n\n\n解码器中的自关注层运行方式与编码器中的运行方式略有不同：\n\n在解码器中，自注意层只允许关注输出序列中较早的位置——通过在自注意计算的softmax步骤前屏蔽未来位置（设置为-inf）。\n\n“Encoder-Decoder Attention”层的工作方式与multiheaded self-attention类似，只不过它是从其下面层创建查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。\n\n最后的线性和Softmax层（The Final Linear and Softmax Layer）\n\n解码器堆栈输出浮点数向量，最后的Linear层将其变成单词，跟随在后面的是softmax层。\n\n线性层是一个简单的全连接神经网络，它将解码器堆栈产生的向量投影到一个更大的向量中，称为logits向量。假设模型知道从训练数据集中学习10000个不同的英文单词（模型的“输出词汇”），将使logits向量有10000个单元格宽度——每个单元格对应一个唯一的分数。然后，softmax层将这些分数转换为概率（全部为正数，加总为1.0）。选择概率最高的单元格，与之相联单词作为该时间步骤的输出结果。\n\n\n从底部开始，生成作为解码器堆栈输出的向量，然后转换为输出单词\n\n回顾训练（Recap Of Training）\n\n在训练模型期间，未经训练的模型将经历完全相同的前向传递，但由于是在标记的训练数据集上进行训练，可以将其输出与实际的证券输出进行比较。\n\n形象化这一点，假设输出词汇仅包含六个单词（”a”, “am”, “i”, “thanks”, “student”, “&lt;\\eos&gt;”(‘end of sentence’)）：\n\n\n模型的输出词汇是在开始训练之前的预处理阶段创建的\n\n一旦定义了输出词汇表，可以使用相同宽度的向量来表示词汇表中的每个单词，称为one-shot编码：\n\n\n输出词汇的one-shot编码\n\n损失函数（The Loss Function）\n\n\n由于模型的参数（权重）都是随机初始化的，（未经训练的）模型会生成每个单元格/单词的任意值概率分布。可以将其与实际输出进行比较，然后使用反向传播调整所有模型的权重，使输出更接近所需的输出\n\n如何比较两个概率分布？只需将其中一个减去另一个即可，可参考cross-entropy 和 Kullback–Leibler divergence。\n\n更现实的是，使用比一个单词长的句子。例如，输入：“je suis étudiant”，预期输出：“i am a student”。意味着希望模型能够连续输出概率分布：\n\n  每个概率宽度由宽度为vocab_size的向量表示\n  第一个概率分布在与单词“i”相关的单元格中具有最高的概率\n  第二个概率分布在与单词“am”相关的单元格中具有更高的概率\n  以此类推，直到第五个输出输出分布“end of sentences”符号，在10000个元素的词汇表中也有一个与之关联的单元格\n\n\n\n\n在足够大的数据集上训练模型足够长的时间后，期望生成的概率分布：\n\n\n\n经过训练后，期望模型能够输出期望的翻译。每个位置都有概率，即使不太可能是该时间步的输出——非常有用的softmax的一个属性，有助于训练过程\n\n此时，模型一次产生一个输出，可以假设模型从该概率分布中选择概率最高的单词，并丢弃其余的单词——贪婪解码方法（greedy decoding）。\n\n另一种方法是保留最上面的两个单词（‘I’，‘a’），在下一步中运行模型两次：一次假设第一个输出位置是‘I’，另一次假设是‘a’，并且考虑位置#1、#2，保留产生较少错误的版本。重复#2、#3等位置——束搜索（beam search）。这些都是可以实验的超参数（hyperparameters）。\n\n延伸阅读\n\nI hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:\n\n\n  Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.\n  Watch Łukasz Kaiser’s talk walking through the model and its details\n  Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo\n  Explore the Tensor2Tensor repo.\n\n\nFollow-up works:\n\n\n  Depthwise Separable Convolutions for Neural Machine Translation\n  One Model To Learn Them All\n  Discrete Autoencoders for Sequence Models\n  Generating Wikipedia by Summarizing Long Sequences\n  Image Transformer\n  Training Tips for the Transformer Model\n  Self-Attention with Relative Position Representations\n  Fast Decoding in Sequence Models using Discrete Latent Variables\n  Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\n\n",
        "url": "/reading/2023/06/30/The-Illustrated-Transformer/"
      },
    
      {
        "title": "MySQL的锁现象及原理",
        "excerpt": "叙述与分析MySQL的一些锁现象和原理\n\n",
        "content": "叙述与分析MySQL的一些锁现象和原理\n\n锁的类别\n\n共享锁与独占锁（Shared and Exclusive Locks）\n\nInnoDB标准的行级锁：共享（S）锁、独占（X）锁\n\n  共享锁允许持有该锁的事务读取一行\n  独占锁允许持有该锁的事务更新或删除行\n\n\n意向锁（Intention Locks）\n\nInnoDB支持多粒度锁定，允许行锁和表锁共存——用意向锁实现。意向锁是表级锁，表明事务稍后需要对表中的行使用哪种类型的锁（共享、独占）。\n\n\n  意向共享锁（IS）表示事务试图在表中的单独行上设置共享锁。\n  意向排它锁（IX）表示事务试图在表中的单独行上设置排它锁。\n\n\nSELECT ... FOR SHARE 设置IS锁，SELECT ... FOR UPDATE 设置IX锁。\n\n意向锁协议：\n\n  在事务获取表中行的共享锁之前，必须先获取表上的IS锁或者更强的锁。\n  在事务获取表中的排它锁之前，必须先获取表的IX锁。\n\n\n表级锁兼容性：\n\n\n  \n    \n       \n      X\n      IX\n      S\n      IS\n    \n  \n  \n    \n      X\n      ❌\n      ❌\n      ❌\n      ❌\n    \n    \n      IX\n      ❌\n      ✅\n      ❌\n      ✅\n    \n    \n      S\n      ❌\n      ❌\n      ✅\n      ✅\n    \n    \n      IS\n      ❌\n      ✅\n      ✅\n      ✅\n    \n  \n\n\n记录锁（Record Locks）\n\n记录锁是索引记录上的锁。记录锁始终锁定索引记录，即使表定义没有索引（InnoDB创建一个隐藏的聚簇索引，并使用该索引进行记录锁定）。\n\n间隙锁（Gap Locks）\n\n间隙锁是对索引记录之间间隙的锁定，或者对第一个索引记录之前或者最后一个索引记录之后的锁定。间隙可能跨越单个、多个索引值，甚至空的。不同事务可以在间隙上持有冲突锁。间隙锁是纯抑制性的（防止其他事务插入间隙），可以共存。一个事务获取的间隙锁不会阻止另一事务在统一间隙上获取间隙锁。\n\n临键锁（Next-Key Locks）\n\n临键锁是索引记录上的记录锁和索引记录之前的间隙上的间隙锁的组合。\n\n索引记录上的临键锁会影响该索引记录之前的“间隙”。如果一个会话对索引中的记录R具有共享锁或独占锁，则另一个会话无法在索引顺序中紧邻R之前的间隙中插入新索引记录。\n\n默认情况，InnoDB在REPEATABLE READ事务隔离级别中生效。InnoDB使用临键锁进行搜索和索引扫描，防止幻影行。\n\n插入意向锁（Insert Intention Locks）\n\n插入意向锁是一种间隙锁，在行插入之前由INSERT设置。插入同一索引间隙的多个事务如果没有插入间隙内的同一位置，则无需互相等待。\n\nAUTO-INC锁（AUTO-INC Locks）\n\nAUTO-INC锁是一种特殊的表级锁，由插入具有AUTO_INCREMENT列的表的事务获取。如果一个事务正在将值插入表中，则任何其他事务必须等待才能向该表中执行自己的插入操作，以便第一个事务插入的行接收连续的主键值。\n\nshared mode lock: 共享模式锁\n\nLocking Reads\n\nhttps://dev.mysql.com/doc/refman/8.0/en/innodb-locking-reads.html\nhttps://shiroyasha.io/selecting-for-share-and-update-in-postgresql.html\n\nSELECT FOR SHARE（LOCK IN SHARE MODE）\n\n对读取的任意行设置共享模式锁，其他会话可以读取这些行，但在事务提交前，无法修改它们。并且如果其中任何一行被其它尚未提交的事务更改，查询将等待直到该事务结束，然后使用最新值。\n\n\n\n  左一会话开启事务，执行FOR SHARE语句，右二会话可以SELECT、FOR SHARE可以获取值。\n  左一会话在事务中更新，右二会话执行普通SELECT得到旧值，执行FOR SHARE进入等待。\n  左一会话提交事务后，右二返回最新值。\n\n\n\n\n  左一开启事务，执行FOR SHARE语句，右二会话执行UPDATE进入等待。\n  左一提交事务，右二返回，SELECT返回最新值。\n\n\nSELECT FOR UPDATE\n\n对于查询的所有行，锁定行和关联的索引条目，与UPDATE语句类似。其他事务将被阻止更新这些行，或者执行FOR SHARE、读取某些事务隔离级别中的数据。\n\n\n\n  左一会话开启事务，执行FOR UPDATE语句，右二会话执行普通语句可以获取值。\n  右二会话执行FOR SHARE/FOR UPDATE进入等待，左二提交事务后，返回值。\n\n\n\n\n  左一会话开启事务，执行FOR UPDATE语句，右二执行UPDATE进入等待。\n  左一提交事务后，右二返回，SELECT返回最新值。\n\n\n当事务提交或回滚时，FOR SHARE和FOR UPDATE设置的所有锁都回被释放。\n\n外部语句的锁定不会锁嵌套子查询表中的行，除非子查询也锁定读取子句。\n\nTheory of SELECT FOR UPDATE\n\n从官方文档上可见，FOR UPDATE的表现形式与UPDATE等效：\n\n\n  A SELECT ... FOR UPDATE reads the latest available data, setting exclusive locks on each row it reads. Thus, it sets the same locks a searched SQL UPDATE would set on the rows.\n\n  The preceding description is merely an example of how SELECT ... FOR UPDATE works. In MySQL, the specific task of generating a unique identifier actually can be accomplished using only a single access to the table:\n\n\nUPDATE child_codes SET counter_field = LAST_INSERT_ID(counter_field + 1);   \nSELECT LAST_INSERT_ID();\n\n\n示例：\n\n\n对于不存在的索引列，附近行（GAP）。\n\n\n对于存在的索引列，会锁住当前行（RECORD）和附近行（GAP）。\n\n\n对于范围查询，会锁住符合条件行（RECORD）和附件行（GAP）。\n\nInnoDB Data Locking\n\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-1-introduction/\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-2-locks/\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-2-5-locks-deeper-dive/\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-3-deadlocks/\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-4-scheduling/\nhttps://dev.mysql.com/blog-archive/innodb-data-locking-part-5-concurrent-queues/\n",
        "url": "/database/2023/07/22/MySQL%E7%9A%84%E9%94%81%E7%8E%B0%E8%B1%A1%E5%8F%8A%E5%8E%9F%E7%90%86/"
      },
    
  
  
  
  {
    "title": "404 - Page not found",
    "excerpt": "\n",
    "content": "Sorry, we can’t find that page that you’re looking for. You can try again by going back to the homepage.\n\n\n",
    "url": "/404.html"
  },
  
  {
    "title": "About",
    "excerpt": "\n",
    "content": "Desnowy’s some thinkings and notes.\n\nMore Information\n\nDo not need much snow.\n\nContact me\n\nemail@domain.com\n",
    "url": "/about/"
  },
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

